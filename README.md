# NanoPrep – Simple Text Preprocessing Tool (LLMOps Beginner Project)

NanoPrep is a simple text preprocessing tool designed for beginners learning LLMOps. It performs text cleaning, tokenization, chunking, and dataset reporting on .txt files. The project is intentionally simple, beginner-friendly, and suitable for Week 1 of an LLMOps learning roadmap.

## Features
NanoPrep performs the core steps used in real LLMOps preprocessing pipelines:
1. Text cleaning
2. Tokenization
3. Chunking
4. Basic dataset report
5. File handling support (local files and Google Drive paths in Colab)

## Why This Project Matters
Before text can be used for LLM training, fine-tuning, or retrieval (RAG), it must be cleaned and structured. NanoPrep demonstrates the foundational preprocessing concepts used in real AI pipelines:
- Preparing raw text
- Normalizing and cleaning input
- Splitting text into tokens
- Breaking documents into context-friendly chunks
- Generating simple dataset statistics
This project is an ideal first step into LLMOps.

## Project Structure
NanoPrep/
│
├── NanoPrep_sample.txt         # Example dataset
├── chunks_output.txt      # Output generated by script
└── nanoprep.py            # Main project code

## How to Use (Google Colab + Google Drive)
1. Mount Google Drive:
from google.colab import drive
drive.mount('/content/drive')

2. Upload your text file to:
My Drive/

3. Run the script and enter the file path when prompted:
Enter text file path:
/content/drive/My Drive/sample.txt

## NanoPrep Code (nanoprep.py)
import string

def clean_text(text):
    table = str.maketrans("", "", string.punctuation)
    return text.lower().translate(table)

def tokenize(text):
    return text.split()

def chunk(tokens, size=50):
    return [tokens[i:i+size] for i in range(0, len(tokens), size)]

def main():
    path = input("Enter text file path: ").strip()

    try:
        with open(path, "r", encoding="utf-8") as f:
            text = f.read()

        cleaned = clean_text(text)
        tokens = tokenize(cleaned)
        chunks = chunk(tokens)

        print("\n=== NanoPrep Report ===")
        print("Total tokens   :", len(tokens))
        print("Unique tokens  :", len(set(tokens)))
        print("Number of chunks:", len(chunks))
        print("First 20 tokens:", tokens[:20])

        with open("chunks_output.txt", "w") as f:
            for i, c in enumerate(chunks, start=1):
                f.write(f"Chunk {i}: {' '.join(c)}\n")

        print("\nChunks saved to chunks_output.txt")

    except Exception as e:
        print("Error:", e)

if __name__ == "__main__":
    main()

## Sample Dataset (sample_big.txt)
Large Language Models (LLMs) are rapidly changing the landscape of artificial intelligence. 
From chatbots to document understanding, code generation, translation, and knowledge retrieval, 
LLMs are quickly becoming the foundation of modern AI applications.

This dataset is designed for testing NanoPrep, a simple text preprocessing tool 
built for absolute beginners in LLMOps. NanoPrep helps you understand the basic 
steps used by production data pipelines that prepare text for LLM training 
or retrieval-augmented generation (RAG) systems.

The primary tasks of preprocessing include:
1. Text cleaning: removing punctuation, normalizing case, and stripping whitespace.
2. Tokenization: splitting text into meaningful units like words or subwords.
3. Chunking: breaking long documents into smaller chunks that fit inside the model context window.
4. Basic analysis: counting total tokens, unique tokens, and chunk distribution.

Real-world text often contains noise. Here are examples:
- Extra   spaces between words.
- Mixed CASES that need normalization.
- Punctuation, such as commas, periods, question marks?
- Line breaks and inconsistent formatting.
- Lists, bullets, and structured content.

LLMOps engineering involves designing tools and workflows that allow large 
language models to run efficiently at scale. It includes dataset creation, cleaning, 
validation, chunking, embedding generation, indexing, evaluation, monitoring, 
and safe deployment of models. Even though this project is small, it mirrors the 
same steps used in enterprise-level pipelines.

NanoPrep is intentionally simple but powerful enough to show the logic behind 
real preprocessing tools. After processing this dataset, you should see:
- Total token count well above 200.
- Multiple chunks created.
- Smooth cleaning and tokenizing across many sentence types.

Artificial intelligence is evolving faster than ever. Companies across the world 
are adopting LLMs for customer support, summarization, automation, and creative insights. 
Text preprocessing plays a vital role in the reliability of these systems because 
the quality of training and inference depends on the quality of input data.

Machine learning teams spend significant time preparing datasets. 
This includes removing noise, standardizing formats, ensuring quality, and making 
text easier for models to understand. Even a simple preprocessing script can 
demonstrate foundational concepts that scale to real industry workflows.

This dataset includes various text structures such as:
- paragraphs
- lists
- line breaks
- long sentences
- short sentences
- irregular formatting

The goal is to give NanoPrep enough diversity to test cleaning, tokenization, 
unique word counting, and chunk formation.

Happy learning, and welcome to your first steps in LLMOps!

## Skills Demonstrated
- Python fundamentals
- Text cleaning
- Tokenization
- Chunking
- File handling
- Dataset reporting
- LLMOps preprocessing basics

## License
MIT License.
